from pydantic import BaseModel, Field
from typing import List, Dict, Any, Optional
from datetime import datetime
from enum import Enum


class PaperSection(str, Enum):
    TITLE = "title"
    ABSTRACT = "abstract"
    INTRODUCTION = "introduction"
    METHODS = "methods"
    RESULTS = "results"
    DISCUSSION = "discussion"
    CONCLUSION = "conclusion"
    REFERENCES = "references"
    APPENDIX = "appendix"


class Author(BaseModel):
    name: str
    affiliation: Optional[str] = None
    email: Optional[str] = None
    orcid: Optional[str] = None
    contribution: Optional[str] = None


class Reference(BaseModel):
    id: str
    authors: List[str]
    title: str
    year: Optional[int] = None
    journal: Optional[str] = None
    volume: Optional[str] = None
    pages: Optional[str] = None
    doi: Optional[str] = None
    url: Optional[str] = None
    arxiv_id: Optional[str] = None


class Figure(BaseModel):
    id: str
    caption: str
    data: Optional[Dict[str, Any]] = None  # Plotly or other figure data
    type: str = "plotly"  # plotly, matplotlib, image, table
    width: Optional[int] = 800
    height: Optional[int] = 600


class Table(BaseModel):
    id: str
    caption: str
    headers: List[str]
    data: List[List[Any]]
    notes: Optional[str] = None


class ConfidenceMetric(BaseModel):
    metric: str
    score: float = Field(ge=0.0, le=1.0)
    explanation: Optional[str] = None


class ResearchPaper(BaseModel):
    """Schema for research paper generated by autonomous system"""

    # Metadata
    session_id: str
    generated_at: datetime = Field(default_factory=datetime.now)
    version: str = "1.0"
    paper_id: Optional[str] = None

    # Title and Authors
    title: str
    authors: List[Author] = []
    corresponding_author: Optional[Author] = None

    # Abstract
    abstract: str
    keywords: List[str] = []

    # Main Content
    sections: Dict[PaperSection, str] = {}

    # Figures and Tables
    figures: List[Figure] = []
    tables: List[Table] = []

    # References
    references: List[Reference] = []

    # Acknowledgments
    acknowledgments: Optional[str] = None

    # Confidence Metrics
    confidence_metrics: List[ConfidenceMetric] = []

    # Limitations (auto-generated by critic)
    limitations: List[str] = []

    # Future Work (auto-generated by critic)
    future_work: List[str] = []

    # Ethical Considerations
    ethical_considerations: Optional[str] = None

    # Data Availability
    data_availability: Optional[str] = None

    # Code Availability
    code_availability: Optional[str] = None

    # Funding Information
    funding: Optional[str] = None

    # Conflicts of Interest
    conflicts_of_interest: Optional[str] = None

    # Additional Metadata
    metadata: Dict[str, Any] = {}

    class Config:
        use_enum_values = True

    def to_markdown(self) -> str:
        """Convert research paper to markdown format"""
        md_parts = []

        # Title
        md_parts.append(f"# {self.title}\n")

        # Authors
        if self.authors:
            author_lines = []
            for author in self.authors:
                line = f"**{author.name}**"
                if author.affiliation:
                    line += f" ({author.affiliation})"
                if author.email:
                    line += f" <{author.email}>"
                author_lines.append(line)

            md_parts.append("## Authors\n")
            md_parts.extend(author_lines)
            md_parts.append("")  # Empty line

        # Abstract
        md_parts.append("## Abstract\n")
        md_parts.append(self.abstract)
        md_parts.append("")  # Empty line

        # Keywords
        if self.keywords:
            md_parts.append(f"**Keywords:** {', '.join(self.keywords)}\n")

        # Sections
        section_order = [
            PaperSection.INTRODUCTION,
            PaperSection.METHODS,
            PaperSection.RESULTS,
            PaperSection.DISCUSSION,
            PaperSection.CONCLUSION
        ]

        for section in section_order:
            if section in self.sections:
                md_parts.append(f"## {section.value.title()}\n")
                md_parts.append(self.sections[section])
                md_parts.append("")  # Empty line

        # Figures
        if self.figures:
            md_parts.append("## Figures\n")
            for figure in self.figures:
                md_parts.append(f"**Figure {figure.id}**: {figure.caption}\n")

                if figure.data and figure.type == "plotly":
                    # For Plotly figures, we'd typically embed HTML or provide a link
                    md_parts.append(f"*Interactive figure available in the web interface*\n")
                md_parts.append("")  # Empty line

        # Tables
        if self.tables:
            md_parts.append("## Tables\n")
            for table in self.tables:
                md_parts.append(f"**Table {table.id}**: {table.caption}\n")

                # Create markdown table
                if table.headers and table.data:
                    # Headers
                    md_parts.append("| " + " | ".join(table.headers) + " |")
                    md_parts.append("|" + "|".join(["---"] * len(table.headers)) + "|")

                    # Data rows
                    for row in table.data:
                        md_parts.append("| " + " | ".join(str(cell) for cell in row) + " |")

                if table.notes:
                    md_parts.append(f"*Note: {table.notes}*")

                md_parts.append("")  # Empty line

        # Limitations
        if self.limitations:
            md_parts.append("## Limitations\n")
            for limitation in self.limitations:
                md_parts.append(f"- {limitation}")
            md_parts.append("")  # Empty line

        # Future Work
        if self.future_work:
            md_parts.append("## Future Work\n")
            for item in self.future_work:
                md_parts.append(f"- {item}")
            md_parts.append("")  # Empty line

        # Confidence Metrics
        if self.confidence_metrics:
            md_parts.append("## Confidence Metrics\n")
            md_parts.append("| Metric | Score | Explanation |")
            md_parts.append("|--------|-------|-------------|")
            for metric in self.confidence_metrics:
                explanation = metric.explanation or ""
                md_parts.append(f"| {metric.metric} | {metric.score:.1%} | {explanation} |")
            md_parts.append("")  # Empty line

        # References
        if self.references:
            md_parts.append("## References\n")
            for i, ref in enumerate(self.references, 1):
                ref_parts = []

                # Authors
                if ref.authors:
                    if len(ref.authors) > 3:
                        authors = f"{ref.authors[0]} et al."
                    else:
                        authors = ", ".join(ref.authors)
                    ref_parts.append(authors)

                # Year
                if ref.year:
                    ref_parts.append(f"({ref.year})")

                # Title
                ref_parts.append(f"**{ref.title}**")

                # Journal/Conference
                if ref.journal:
                    ref_parts.append(f"*{ref.journal}*")

                # Volume and pages
                volume_info = []
                if ref.volume:
                    volume_info.append(f"**{ref.volume}**")
                if ref.pages:
                    volume_info.append(ref.pages)
                if volume_info:
                    ref_parts.append(", ".join(volume_info))

                # DOI/URL
                if ref.doi:
                    ref_parts.append(f"DOI: {ref.doi}")
                elif ref.url:
                    ref_parts.append(f"URL: {ref.url}")
                elif ref.arxiv_id:
                    ref_parts.append(f"arXiv: {ref.arxiv_id}")

                md_parts.append(f"{i}. " + ". ".join(ref_parts))

        # Acknowledgments
        if self.acknowledgments:
            md_parts.append("\n## Acknowledgments\n")
            md_parts.append(self.acknowledgments)

        # Footer
        md_parts.append("\n---\n")
        md_parts.append(f"*Autonomously generated by AI Research Assistant*  \n")
        md_parts.append(f"*Session ID: {self.session_id}*  \n")
        md_parts.append(f"*Generated: {self.generated_at.isoformat()}*  \n")
        md_parts.append(f"*Version: {self.version}*")

        return "\n".join(md_parts)

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for JSON serialization"""
        return self.dict()

    @classmethod
    def from_autonomous_output(cls, session_id: str,
                               autonomous_output: Dict[str, Any]) -> 'ResearchPaper':
        """Create ResearchPaper from autonomous system output"""

        # Extract information from autonomous output
        domains = autonomous_output.get("domains", {})
        questions = autonomous_output.get("questions", {})
        data = autonomous_output.get("data", {})
        experiments = autonomous_output.get("experiments", {})
        critique = autonomous_output.get("critique", {})
        uncertainty = autonomous_output.get("uncertainty", {})

        # Generate title
        title = cls._generate_title(domains, questions)

        # Generate abstract
        abstract = cls._generate_abstract(domains, questions, data, experiments)

        # Generate sections
        sections = cls._generate_sections(domains, questions, data, experiments, critique)

        # Generate confidence metrics
        confidence_metrics = cls._generate_confidence_metrics(uncertainty)

        # Generate limitations and future work from critique
        limitations, future_work = cls._extract_critique_insights(critique)

        # Create paper
        paper = cls(
            session_id=session_id,
            title=title,
            abstract=abstract,
            sections=sections,
            confidence_metrics=confidence_metrics,
            limitations=limitations,
            future_work=future_work,
            # Set default values for other fields
            authors=[Author(name="Autonomous AI Research System")],
            keywords=cls._extract_keywords(domains, questions),
            metadata={
                "autonomous_generation": True,
                "source_agents": ["domain_scout", "question_generator", "data_alchemist",
                                  "experiment_designer", "critic", "uncertainty"],
                "iteration_count": autonomous_output.get("total_iterations", 1)
            }
        )

        return paper

    @staticmethod
    def _generate_title(domains: Dict[str, Any], questions: Dict[str, Any]) -> str:
        """Generate paper title from domains and questions"""
        if domains and domains.get("domains"):
            best_domain = domains["domains"][0].get("name", "Emerging Science") if domains[
                "domains"] else "Emerging Science"

            if questions and questions.get("questions"):
                best_question = questions["questions"][0].get("question", "") if questions["questions"] else ""

                # Extract key terms from question
                import re
                key_terms = re.findall(r'\b[A-Z][a-z]+\b', best_question)

                if key_terms and len(key_terms) >= 2:
                    return f"{key_terms[0]} and {key_terms[1]} in {best_domain}: An Autonomous Investigation"

            return f"Autonomous Research in {best_domain}: AI-Driven Discovery"

        return "Autonomous AI Research: System-Generated Investigation"

    @staticmethod
    def _generate_abstract(domains: Dict[str, Any], questions: Dict[str, Any],
                           data: Dict[str, Any], experiments: Dict[str, Any]) -> str:
        """Generate abstract from research components"""
        abstract_parts = []

        # Domain context
        if domains and domains.get("domains"):
            domain_names = [d.get("name", "") for d in domains["domains"][:2]]
            if domain_names:
                abstract_parts.append(f"This paper explores emerging research in {', '.join(domain_names)}.")

        # Research question
        if questions and questions.get("questions"):
            best_question = questions["questions"][0].get("question", "") if questions["questions"] else ""
            if best_question:
                abstract_parts.append(f"We investigate: '{best_question}'")

        # Methodology
        if experiments:
            abstract_parts.append(
                "Using autonomous AI agents, we design experiments and analyze data from multiple sources.")

        # Key finding
        abstract_parts.append(
            "This research demonstrates the potential of fully autonomous AI systems for scientific discovery.")

        # Conclusion
        abstract_parts.append(
            "Results show that AI can autonomously conduct end-to-end research with minimal human intervention.")

        return " ".join(abstract_parts)

    @staticmethod
    def _generate_sections(domains: Dict[str, Any], questions: Dict[str, Any],
                           data: Dict[str, Any], experiments: Dict[str, Any],
                           critique: Dict[str, Any]) -> Dict[PaperSection, str]:
        """Generate paper sections from research components"""
        sections = {}

        # Introduction
        intro = ResearchPaper._generate_introduction(domains, questions)
        sections[PaperSection.INTRODUCTION] = intro

        # Methods
        methods = ResearchPaper._generate_methods(data, experiments)
        sections[PaperSection.METHODS] = methods

        # Results
        results = ResearchPaper._generate_results(domains, questions, data, experiments)
        sections[PaperSection.RESULTS] = results

        # Discussion
        discussion = ResearchPaper._generate_discussion(domains, questions, experiments, critique)
        sections[PaperSection.DISCUSSION] = discussion

        # Conclusion
        conclusion = ResearchPaper._generate_conclusion(domains, experiments, critique)
        sections[PaperSection.CONCLUSION] = conclusion

        return sections

    @staticmethod
    def _generate_introduction(domains: Dict[str, Any], questions: Dict[str, Any]) -> str:
        """Generate introduction section"""
        intro_parts = ["# Introduction\n"]

        # Domain background
        if domains and domains.get("domains"):
            domain_list = domains["domains"][:3]  # Top 3 domains
            intro_parts.append("## Emerging Research Domains\n")

            for domain in domain_list:
                name = domain.get("name", "Unknown")
                confidence = domain.get("confidence", 0.5)
                evidence_count = len(domain.get("evidence", []))

                intro_parts.append(f"### {name}")
                intro_parts.append(f"Identified with {confidence:.0%} confidence based on {evidence_count} sources.")

                if domain.get("evidence"):
                    evidence_preview = domain["evidence"][0][:200] + "..." if len(domain["evidence"][0]) > 200 else \
                    domain["evidence"][0]
                    intro_parts.append(f"Evidence: {evidence_preview}")

                intro_parts.append("")  # Empty line

        # Research questions
        if questions and questions.get("questions"):
            intro_parts.append("## Research Questions\n")

            question_list = questions["questions"][:3]  # Top 3 questions
            for i, question in enumerate(question_list, 1):
                q_text = question.get("question", "")
                novelty = question.get("novelty_score", 0.5)
                feasibility = question.get("feasibility_score", 0.5)

                intro_parts.append(f"{i}. **{q_text}**")
                intro_parts.append(f"   - Novelty: {novelty:.0%}")
                intro_parts.append(f"   - Feasibility: {feasibility:.0%}")

                if question.get("explanation"):
                    intro_parts.append(f"   - Rationale: {question['explanation']}")

                intro_parts.append("")  # Empty line

        # Research approach
        intro_parts.append("## Autonomous Research Approach\n")
        intro_parts.append("This research was conducted entirely by an autonomous AI system consisting of:")
        intro_parts.append("- Domain discovery agents")
        intro_parts.append("- Question generation agents")
        intro_parts.append("- Data acquisition and cleaning agents")
        intro_parts.append("- Experimental design agents")
        intro_parts.append("- Critical review agents")
        intro_parts.append("- Uncertainty quantification agents")
        intro_parts.append("")
        intro_parts.append("The system operates without human intervention after initialization.")

        return "\n".join(intro_parts)

    @staticmethod
    def _generate_methods(data: Dict[str, Any], experiments: Dict[str, Any]) -> str:
        """Generate methods section"""
        methods_parts = ["# Methods\n"]

        methods_parts.append("## Autonomous Research System Architecture\n")
        methods_parts.append(
            "The research was conducted using a multi-agent AI system with the following components:\n")

        methods_parts.append("### 1. Domain Scout Agent")
        methods_parts.append("- Discovers emerging scientific fields post-2024")
        methods_parts.append("- Uses real-time search across academic and public sources")
        methods_parts.append("- Evaluates domain novelty and momentum")
        methods_parts.append("")

        methods_parts.append("### 2. Question Generator Agent")
        methods_parts.append("- Formulates novel research questions")
        methods_parts.append("- Assesses question novelty and feasibility")
        methods_parts.append("- Ensures questions require synthesis from multiple sources")
        methods_parts.append("")

        methods_parts.append("### 3. Data Alchemist Agent")
        methods_parts.append("- Acquires data from â‰¥3 disparate sources")
        methods_parts.append("- Cleans and integrates heterogeneous data")
        methods_parts.append("- Assesses data quality and relevance")
        methods_parts.append("")

        methods_parts.append("### 4. Experiment Designer Agent")
        methods_parts.append("- Designs experiments to test hypotheses")
        methods_parts.append("- Selects appropriate statistical methods")
        methods_parts.append("- Calculates required sample sizes")
        methods_parts.append("")

        methods_parts.append("### 5. Critic Agent")
        methods_parts.append("- Critiques methodology and assumptions")
        methods_parts.append("- Identifies statistical issues")
        methods_parts.append("- Forces iteration if p-value > 0.05 or effect size trivial")
        methods_parts.append("")

        methods_parts.append("### 6. Uncertainty Quantification Agent")
        methods_parts.append("- Quantifies confidence in all outputs")
        methods_parts.append("- Forces abstention if confidence < 60%")
        methods_parts.append("- Propagates errors through the pipeline")
        methods_parts.append("")

        # Data collection methods
        if data:
            methods_parts.append("## Data Collection\n")

            sources = data.get("sources", [])
            methods_parts.append(f"Data was collected from {len(sources)} sources:")

            for i, source in enumerate(sources[:5], 1):  # Top 5 sources
                source_type = source.get("type", "Unknown")
                description = source.get("description", "")
                methods_parts.append(f"{i}. **{source_type}**: {description[:200]}...")

            quality_metrics = data.get("quality_metrics", {})
            if quality_metrics:
                overall_quality = quality_metrics.get("overall_quality", 0.5)
                methods_parts.append(f"\nOverall data quality score: {overall_quality:.0%}")
            methods_parts.append("")

        # Experimental design
        if experiments:
            methods_parts.append("## Experimental Design\n")

            primary_experiment = experiments.get("primary_experiment", {})
            if primary_experiment:
                exp_type = primary_experiment.get("type", "Observational")
                methods_parts.append(f"**Experiment Type**: {exp_type}")

                methodology = primary_experiment.get("methodology", {})
                if isinstance(methodology, dict) and methodology.get("description"):
                    methods_parts.append(f"**Methodology**: {methodology['description'][:500]}...")

                # Statistical methods
                stat_methods = primary_experiment.get("statistical_methods", [])
                if stat_methods:
                    methods_parts.append("\n**Statistical Methods**:")
                    for method in stat_methods[:3]:
                        if isinstance(method, dict):
                            name = method.get("name", "Unknown")
                            purpose = method.get("purpose", "")
                            methods_parts.append(f"- **{name}**: {purpose[:200]}...")

                # Sample size
                sample_size = primary_experiment.get("sample_size", {})
                if sample_size:
                    min_size = sample_size.get("minimum_sample_size", "Not specified")
                    methods_parts.append(f"\n**Sample Size**: Minimum {min_size} required")

            methods_parts.append("")

        return "\n".join(methods_parts)

    @staticmethod
    def _generate_results(domains: Dict[str, Any], questions: Dict[str, Any],
                          data: Dict[str, Any], experiments: Dict[str, Any]) -> str:
        """Generate results section"""
        results_parts = ["# Results\n"]

        # Domain discovery results
        if domains and domains.get("domains"):
            results_parts.append("## Domain Discovery Results\n")

            domain_list = domains["domains"][:3]  # Top 3 domains
            for domain in domain_list:
                name = domain.get("name", "Unknown")
                confidence = domain.get("confidence", 0.5)
                source_count = len(domain.get("sources", []))
                evidence_count = len(domain.get("evidence", []))

                results_parts.append(f"### {name}")
                results_parts.append(f"- Confidence: {confidence:.0%}")
                results_parts.append(f"- Sources: {source_count}")
                results_parts.append(f"- Evidence points: {evidence_count}")

                if domain.get("evidence"):
                    # Show first evidence point
                    evidence = domain["evidence"][0]
                    if len(evidence) > 300:
                        evidence = evidence[:300] + "..."
                    results_parts.append(f"- Key evidence: {evidence}")

                results_parts.append("")

        # Question generation results
        if questions and questions.get("questions"):
            results_parts.append("## Research Question Analysis\n")

            question_list = questions["questions"][:3]  # Top 3 questions
            results_parts.append("The system generated the following research questions:\n")

            for i, question in enumerate(question_list, 1):
                q_text = question.get("question", "")
                novelty = question.get("novelty_score", 0.5)
                feasibility = question.get("feasibility_score", 0.5)
                overall = question.get("overall_score", 0.5)

                results_parts.append(f"{i}. **{q_text}**")
                results_parts.append(f"   - Overall score: {overall:.0%}")
                results_parts.append(f"   - Novelty: {novelty:.0%}")
                results_parts.append(f"   - Feasibility: {feasibility:.0%}")

                if question.get("required_data"):
                    results_parts.append(f"   - Required data: {question['required_data'][:200]}...")

                results_parts.append("")

        # Data acquisition results
        if data:
            results_parts.append("## Data Acquisition Results\n")

            sources = data.get("sources", [])
            results_parts.append(f"Successfully acquired data from {len(sources)} sources.\n")

            quality_metrics = data.get("quality_metrics", {})
            if quality_metrics:
                overall_quality = quality_metrics.get("overall_quality", 0.5)
                results_parts.append(f"**Data Quality Assessment**:")
                results_parts.append(f"- Overall quality: {overall_quality:.0%}")

                # List other quality metrics
                for key, value in quality_metrics.items():
                    if key != "overall_quality" and isinstance(value, (int, float)):
                        results_parts.append(f"- {key.replace('_', ' ').title()}: {value:.0%}")

            results_parts.append("")

        # Experimental design results
        if experiments:
            results_parts.append("## Experimental Design Results\n")

            primary_experiment = experiments.get("primary_experiment", {})
            if primary_experiment:
                exp_type = primary_experiment.get("type", "Unknown")
                complexity = primary_experiment.get("complexity", 0.5)

                results_parts.append(f"**Primary Experiment**: {exp_type}")
                results_parts.append(f"- Complexity score: {complexity:.0%}")

                feasibility = experiments.get("feasibility_assessment", {})
                if feasibility:
                    overall_feasibility = feasibility.get("overall_feasibility", 0.5)
                    results_parts.append(f"- Overall feasibility: {overall_feasibility:.0%}")

                # Statistical methods
                stat_methods = primary_experiment.get("statistical_methods", [])
                if stat_methods:
                    results_parts.append("\n**Selected Statistical Methods**:")
                    for method in stat_methods[:3]:
                        if isinstance(method, dict):
                            name = method.get("name", "Unknown")
                            appropriateness = method.get("appropriateness_score", 0.5)
                            results_parts.append(f"- {name} (appropriateness: {appropriateness:.0%})")

            results_parts.append("")

        return "\n".join(results_parts)

    @staticmethod
    def _generate_discussion(domains: Dict[str, Any], questions: Dict[str, Any],
                             experiments: Dict[str, Any], critique: Dict[str, Any]) -> str:
        """Generate discussion section"""
        discussion_parts = ["# Discussion\n"]

        discussion_parts.append("## Interpretation of Results\n")

        # Domain discovery implications
        if domains and domains.get("domains"):
            discussion_parts.append("### Implications of Domain Discovery")
            discussion_parts.append("The autonomous system successfully identified emerging research domains.")
            discussion_parts.append("This demonstrates AI's capability to detect scientific trends and ")
            discussion_parts.append("identify promising research directions without human guidance.")
            discussion_parts.append("")

        # Question generation implications
        if questions and questions.get("questions"):
            discussion_parts.append("### Quality of Generated Questions")
            discussion_parts.append("The research questions generated by the AI system show:")
            discussion_parts.append("- Appropriate novelty for emerging fields")
            discussion_parts.append("- Practical feasibility given available data")
            discussion_parts.append("- Requirement for synthesis across sources")
            discussion_parts.append("This suggests AI can formulate meaningful research questions.")
            discussion_parts.append("")

        # Experimental design implications
        if experiments:
            discussion_parts.append("### Experimental Design Considerations")
            discussion_parts.append("The autonomously designed experiments demonstrate:")
            discussion_parts.append("- Appropriate methodology selection")
            discussion_parts.append("- Consideration of statistical requirements")
            discussion_parts.append("- Attention to practical feasibility")
            discussion_parts.append("However, human review remains essential for complex designs.")
            discussion_parts.append("")

        # Critique insights
        if critique:
            discussion_parts.append("### Critical Review Insights")

            requires_iteration = critique.get("requires_iteration", False)
            if requires_iteration:
                discussion_parts.append("The critic agent identified issues requiring iteration.")
                discussion_parts.append("This demonstrates the system's capacity for self-correction.")
            else:
                discussion_parts.append("The critic agent found no major issues requiring iteration.")
                discussion_parts.append("This suggests the research design meets basic quality standards.")

            major_issues = critique.get("major_issues", [])
            if major_issues:
                discussion_parts.append("\n**Major issues identified**:")
                for issue in major_issues[:3]:
                    discussion_parts.append(f"- {issue}")

            discussion_parts.append("")

        # Broader implications
        discussion_parts.append("## Broader Implications\n")
        discussion_parts.append("### For Scientific Research")
        discussion_parts.append("1. **Accelerated Discovery**: AI can explore vast research spaces quickly")
        discussion_parts.append("2. **Novel Perspectives**: AI may identify connections humans might miss")
        discussion_parts.append("3. **Resource Efficiency**: Reduces human effort in initial exploration")
        discussion_parts.append("4. **Reproducibility**: Precise documentation of AI methodology")
        discussion_parts.append("")

        discussion_parts.append("### For AI Development")
        discussion_parts.append("1. **Agent Collaboration**: Multiple specialized agents can work together")
        discussion_parts.append("2. **Self-Correction**: Critic agents enable iterative improvement")
        discussion_parts.append("3. **Uncertainty Awareness**: Confidence quantification prevents overconfidence")
        discussion_parts.append("4. **End-to-End Automation**: Complete research pipeline automation")
        discussion_parts.append("")

        return "\n".join(discussion_parts)

    @staticmethod
    def _generate_conclusion(domains: Dict[str, Any], experiments: Dict[str, Any],
                             critique: Dict[str, Any]) -> str:
        """Generate conclusion section"""
        conclusion_parts = ["# Conclusion\n"]

        conclusion_parts.append("This research demonstrates that autonomous AI systems can:")
        conclusion_parts.append("")
        conclusion_parts.append("1. **Discover** emerging scientific domains")
        conclusion_parts.append("2. **Formulate** novel research questions")
        conclusion_parts.append("3. **Acquire** and **process** relevant data")
        conclusion_parts.append("4. **Design** and **critique** experiments")
        conclusion_parts.append("5. **Generate** coherent research papers")
        conclusion_parts.append("")

        conclusion_parts.append("## Key Achievements\n")

        if domains and domains.get("domains"):
            domain_count = len(domains.get("domains", []))
            conclusion_parts.append(f"- Identified {domain_count} emerging research domains")

        if experiments:
            conclusion_parts.append("- Designed feasible experimental approaches")

        if critique:
            requires_iteration = critique.get("requires_iteration", False)
            if not requires_iteration:
                conclusion_parts.append("- Passed critical review without requiring iteration")

        conclusion_parts.append("")
        conclusion_parts.append("## Future Outlook\n")
        conclusion_parts.append("While this system represents significant progress in autonomous research, ")
        conclusion_parts.append("future work should focus on:")
        conclusion_parts.append("1. Integration with physical experimental setups")
        conclusion_parts.append("2. Handling more complex data types and modalities")
        conclusion_parts.append("3. Improved collaboration between AI and human researchers")
        conclusion_parts.append("4. Ethical frameworks for autonomous scientific discovery")
        conclusion_parts.append("")
        conclusion_parts.append("Autonomous AI research systems have the potential to revolutionize ")
        conclusion_parts.append("scientific discovery by accelerating exploration and enabling ")
        conclusion_parts.append("novel research directions that might otherwise remain unexplored.")

        return "\n".join(conclusion_parts)

    @staticmethod
    def _generate_confidence_metrics(uncertainty: Dict[str, Any]) -> List[ConfidenceMetric]:
        """Generate confidence metrics from uncertainty analysis"""
        metrics = []

        if not uncertainty:
            return metrics

        uncertainty_analysis = uncertainty.get("uncertainty_analysis", {})

        # Overall confidence
        overall_confidence = uncertainty_analysis.get("overall_confidence", 0.5)
        metrics.append(ConfidenceMetric(
            metric="Overall Research Confidence",
            score=overall_confidence,
            explanation="Overall confidence in the complete research pipeline"
        ))

        # Domain confidence
        confidence_scores = uncertainty_analysis.get("confidence_scores", {})
        if isinstance(confidence_scores, dict):
            for key, value in confidence_scores.items():
                if isinstance(value, (int, float)):
                    metric_name = key.replace('_', ' ').title()
                    metrics.append(ConfidenceMetric(
                        metric=metric_name,
                        score=value
                    ))

        return metrics

    @staticmethod
    def _extract_critique_insights(critique: Dict[str, Any]) -> tuple:
        """Extract limitations and future work from critique"""
        limitations = []
        future_work = []

        if not critique:
            return limitations, future_work

        # Extract limitations from critique issues
        issues_found = critique.get("issues_found", [])
        for issue in issues_found[:5]:  # Top 5 issues as limitations
            limitations.append(issue)

        # Extract suggestions for future work
        suggestions = critique.get("suggestions", [])
        for suggestion in suggestions[:5]:  # Top 5 suggestions as future work
            future_work.append(suggestion)

        # If no specific issues/suggestions, add generic ones
        if not limitations:
            limitations = [
                "Reliance on publicly available data may limit depth",
                "AI lacks deep domain-specific knowledge of human experts",
                "Experimental validation requires human execution",
                "Ethical review requires human oversight"
            ]

        if not future_work:
            future_work = [
                "Integrate with physical experimental setups",
                "Incorporate multi-modal data (images, videos, sensors)",
                "Develop hybrid AI-human collaborative systems",
                "Create ethical frameworks for autonomous research"
            ]

        return limitations, future_work

    @staticmethod
    def _extract_keywords(domains: Dict[str, Any], questions: Dict[str, Any]) -> List[str]:
        """Extract keywords from domains and questions"""
        keywords = set()

        # Add domain names as keywords
        if domains and domains.get("domains"):
            for domain in domains["domains"][:3]:
                name = domain.get("name", "")
                if name:
                    # Split compound domain names
                    name_parts = name.split()
                    keywords.update(name_parts)

        # Add terms from questions
        if questions and questions.get("questions"):
            import re
            for question in questions["questions"][:2]:
                q_text = question.get("question", "")
                # Extract capitalized words and key terms
                key_terms = re.findall(r'\b[A-Z][a-z]+\b', q_text)
                keywords.update(key_terms)

                # Also add longer words
                long_words = re.findall(r'\b\w{6,}\b', q_text.lower())
                keywords.update(long_words[:5])

        # Add standard keywords
        keywords.update(["autonomous", "AI", "research", "scientific", "discovery", "machine learning"])

        return list(keywords)[:10]  # Limit to 10 keywords